{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b_tWTEetDZqQ",
        "NlmsjfDLFQse",
        "a1n3zmh5Jw9x",
        "IPPdRPm9HBgy",
        "xVHhZ3OVHTm1",
        "6msVLevwcRhm",
        "C35qI15lgI19"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysarathy99/Learning-Journal/blob/main/YS_NNet_WorkPad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nWetWWd_ns"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pHVBk_seED1"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "id": "-CgfuiGANpJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "id": "IVcIQHr1Pfn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get a Kaggle account using your xNeurals email and download the json (use it below)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "PWMduRtUQ4vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()\n"
      ],
      "metadata": {
        "id": "jymw1OOiQ7Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -d dataset faces.zip"
      ],
      "metadata": {
        "id": "2KJruc-qSwNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "landmarks_frame = pd.read_csv('dataset/faces/face_landmarks.csv')\n",
        "\n",
        "n = 65\n",
        "img_name = landmarks_frame.iloc[n, 0]\n",
        "landmarks = landmarks_frame.iloc[n, 1:]\n",
        "landmarks = np.asarray(landmarks)\n",
        "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "\n",
        "print('Image name: {}'.format(img_name))\n",
        "print('Landmarks shape: {}'.format(landmarks.shape))\n",
        "print('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ],
      "metadata": {
        "id": "lqNqysW-TD9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_landmarks(image, landmarks):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "plt.figure()\n",
        "show_landmarks(io.imread(os.path.join('dataset/faces/', img_name)),\n",
        "               landmarks)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B4raYAMLTT73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceLandmarksDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "t5u0lxQSU9jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_dataset = FaceLandmarksDataset(csv_file='dataset/faces/face_landmarks.csv',\n",
        "                                    root_dir='dataset/faces/')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(face_dataset)):\n",
        "    sample = face_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
        "\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    ax.axis('off')\n",
        "    show_landmarks(**sample)\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break"
      ],
      "metadata": {
        "id": "qfdL6eXYVElV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "\n",
        "        # h and w are swapped for landmarks because for images,\n",
        "        # x and y axes are axis 1 and 0 respectively\n",
        "        landmarks = landmarks * [new_w / w, new_h / h]\n",
        "\n",
        "        return {'image': img, 'landmarks': landmarks}\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = np.random.randint(0, h - new_h)\n",
        "        left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "        landmarks = landmarks - [left, top]\n",
        "\n",
        "        return {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'landmarks': torch.from_numpy(landmarks)}"
      ],
      "metadata": {
        "id": "Fye-MgCjVQ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale = Rescale(256)\n",
        "crop = RandomCrop(128)\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "\n",
        "# Apply each of the above transforms on sample.\n",
        "fig = plt.figure()\n",
        "sample = face_dataset[65]\n",
        "for i, tsfrm in enumerate([scale, crop, composed]):\n",
        "    transformed_sample = tsfrm(sample)\n",
        "\n",
        "    ax = plt.subplot(1, 3, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title(type(tsfrm).__name__)\n",
        "    show_landmarks(**transformed_sample)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4IUCmm3hVUjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_dataset = FaceLandmarksDataset(csv_file='dataset/faces/face_landmarks.csv',\n",
        "                                           root_dir='dataset/faces/',\n",
        "                                           transform=transforms.Compose([\n",
        "                                               Rescale(256),\n",
        "                                               RandomCrop(224),\n",
        "                                               ToTensor()\n",
        "                                           ]))\n",
        "\n",
        "for i in range(len(transformed_dataset)):\n",
        "    sample = transformed_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].size(), sample['landmarks'].size())\n",
        "\n",
        "    if i == 3:\n",
        "        break"
      ],
      "metadata": {
        "id": "3L8-q2I_Velk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "# Helper function to show a batch\n",
        "def show_landmarks_batch(sample_batched):\n",
        "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
        "    images_batch, landmarks_batch = \\\n",
        "            sample_batched['image'], sample_batched['landmarks']\n",
        "    batch_size = len(images_batch)\n",
        "    im_size = images_batch.size(2)\n",
        "    grid_border_size = 2\n",
        "\n",
        "    grid = utils.make_grid(images_batch)\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n",
        "                    landmarks_batch[i, :, 1].numpy() + grid_border_size,\n",
        "                    s=10, marker='.', c='r')\n",
        "\n",
        "        plt.title('Batch from dataloader')\n",
        "\n",
        "# if you are using Windows, uncomment the next line and indent the for loop.\n",
        "# you might need to go back and change \"num_workers\" to 0.\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "    print(i_batch, sample_batched['image'].size(),\n",
        "          sample_batched['landmarks'].size())\n",
        "\n",
        "    # observe 4th batch and stop.\n",
        "    if i_batch == 3:\n",
        "        plt.figure()\n",
        "        show_landmarks_batch(sample_batched)\n",
        "        plt.axis('off')\n",
        "        plt.ioff()\n",
        "        plt.show()\n",
        "        break"
      ],
      "metadata": {
        "id": "JvItktAUVmPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODO8PNuaDJKP"
      },
      "source": [
        "\n",
        "# **AuxGAN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_tWTEetDZqQ"
      },
      "source": [
        "##**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcSpbGF7m07L"
      },
      "source": [
        "# example of fitting an auxiliary classifier gan (ac-gan) on fashion mnsit\n",
        "\n",
        "# NUMPY Imports\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import expand_dims\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "\n",
        "\n",
        "# Keras, built-in MNIST Import\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "\n",
        "# Keras Model, Optimizer, Initializer\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# Keras Layers: Input,Dense, Reshape, Flatten\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "\n",
        "# Keras Layers: Conv2D, Transpose\n",
        "from keras.layers import Conv2D, Conv2DTranspose\n",
        "\n",
        "# Keras Layers: LeakyReLU, BatchNorm,Dropout,Embed,Activation, Concat\n",
        "from keras.layers import LeakyReLU, BatchNormalization, Dropout, Embedding, Activation, Concatenate\n",
        "\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmsjfDLFQse"
      },
      "source": [
        "## **Def Descriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QhxAY_FnzyR"
      },
      "source": [
        "\n",
        "# define the standalone discriminator model\n",
        "def define_discriminator(InPutShape=(28,28,1), n_classes=10):\n",
        "\t# weight initialization\n",
        "\tWeightInit = RandomNormal(stddev=0.02)\n",
        "\t# image input\n",
        "\tInPut_Image = Input(shape=InPutShape)\n",
        "\n",
        "    # downsample to 14x14\n",
        "\tDisci = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=WeightInit)(InPut_Image)\n",
        "\tDisci = LeakyReLU(alpha=0.2)(Disci)\n",
        "\tDisci = Dropout(0.5)(Disci)\n",
        "\t# normal\n",
        "\tDisci = Conv2D(64, (3,3), padding='same', kernel_initializer=WeightInit)(Disci)\n",
        "\tDisci = BatchNormalization()(Disci)\n",
        "\tDisci = LeakyReLU(alpha=0.2)(Disci)\n",
        "\tDisci = Dropout(0.5)(Disci)\n",
        "\t# downsample to 7x7\n",
        "\tDisci = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=WeightInit)(Disci)\n",
        "\tDisci = BatchNormalization()(Disci)\n",
        "\tDisci = LeakyReLU(alpha=0.2)(Disci)\n",
        "\tDisci = Dropout(0.5)(Disci)\n",
        "\t# normal\n",
        "\tDisci = Conv2D(256, (3,3), padding='same', kernel_initializer=WeightInit)(Disci)\n",
        "\tDisci = BatchNormalization()(Disci)\n",
        "\tDisci = LeakyReLU(alpha=0.2)(Disci)\n",
        "\tDisci = Dropout(0.5)(Disci)\n",
        "\t# flatten Disciature maps\n",
        "\tDisci = Flatten()(Disci)\n",
        "\t# real/fake output\n",
        "\tout1 = Dense(1, activation='sigmoid')(Disci)\n",
        "\t# class label output\n",
        "\tout2 = Dense(n_classes, activation='softmax')(Disci)\n",
        "\t# define model\n",
        "\tmodel = Model(InPut_Image, [out1, out2])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
        "\treturn model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnucDpskGKWq"
      },
      "source": [
        "## **Load Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyFJ713rGUJN"
      },
      "source": [
        "# load images\n",
        "def load_real_samples():\n",
        "\t# load dataset\n",
        "\t(trainX, trainy), (_, _) = load_data()\n",
        "\t# expand to 3d, e.g. add channels\n",
        "\tX = expand_dims(trainX, axis=-1)\n",
        "\t# convert from ints to floats\n",
        "\tX = X.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\tprint(X.shape, trainy.shape)\n",
        "\treturn [X, trainy]\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# split into images and labels\n",
        "\timages, labels = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, images.shape[0], n_samples)\n",
        "\t# select images and labels\n",
        "\tX, labels = images[ix], labels[ix]\n",
        "\t# generate class labels\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn [X, labels], y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\t# generate labels\n",
        "\tlabels = randint(0, n_classes, n_samples)\n",
        "\treturn [z_input, labels]\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\t# create class labels\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn [images, labels_input], y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'\n",
        "data = \"X-DSPAM-Confidence:    0.8475\"\n",
        "pos = data.find('.')\n",
        "print(data[pos:pos+3])\n",
        "print (pos)"
      ],
      "metadata": {
        "id": "bclCUMLtZUo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiswVeaFGe7o"
      },
      "source": [
        "## **Define Generator & GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNfYu_yiHq58"
      },
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, n_classes=10):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# label input\n",
        "\tInputLabel = Input(shape=(1,))\n",
        "\t# embedding for categorical input\n",
        "\tli = Embedding(n_classes, 50)(InputLabel)\n",
        "\t# linear multiplication\n",
        "\tn_nodes = 7 * 7\n",
        "\tli = Dense(n_nodes, kernel_initializer=init)(li)\n",
        "\t# reshape to additional channel\n",
        "\tli = Reshape((7, 7, 1))(li)\n",
        "\t# image generator input\n",
        "\tin_lat = Input(shape=(latent_dim,))\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 384 * 7 * 7\n",
        "\tgen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
        "\tgen = Activation('relu')(gen)\n",
        "\tgen = Reshape((7, 7, 384))(gen)\n",
        "\t# merge image gen and label input\n",
        "\tmerge = Concatenate()([gen, li])\n",
        "\t# upsample to 14x14\n",
        "\tgen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
        "\tgen = BatchNormalization()(gen)\n",
        "\tgen = Activation('relu')(gen)\n",
        "\t# upsample to 28x28\n",
        "\tgen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
        "\tout_layer = Activation('tanh')(gen)\n",
        "\t# define model\n",
        "\tmodel = Model([in_lat, InputLabel], out_layer)\n",
        "\treturn model\n",
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False\n",
        "\t# connect the outputs of the generator to the inputs of the discriminator\n",
        "\tgan_output = d_model(g_model.output)\n",
        "\t# define gan model as taking noise and label and outputting real/fake and label outputs\n",
        "\tmodel = Model(g_model.input, gan_output)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1n3zmh5Jw9x"
      },
      "source": [
        "## Summarize Perf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB2qRD1lJ2R1"
      },
      "source": [
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
        "\t# prepare fake examples\n",
        "\t[X, _], _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\tX = (X + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(100):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(10, 10, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n",
        "\tpyplot.savefig(filename1)\n",
        "\tpyplot.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%04d.h5' % (step+1)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPPdRPm9HBgy"
      },
      "source": [
        "## **Train**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0do3i1nHeqM"
      },
      "source": [
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=64):\n",
        "\t# calculate the size of the training batches (in each epoch)\n",
        "\tbatSize_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = batSize_per_epo * n_epochs\n",
        "\t# calculate the size of half a batch of samples\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# get randomly selected 'real' samples\n",
        "\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t# update discriminator model weights\n",
        "\t\t_,d_r1,d_r2 = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
        "\t\t# generate 'fake' examples\n",
        "\t\t[X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t# update discriminator model weights\n",
        "\t\t_,d_f,d_f2 = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\t[z_input, z_labels] = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t# update the generator via the discriminator's error\n",
        "\t\t_,g_1,g_2 = gan_model.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
        "\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_r1,d_r2, d_f,d_f2, g_1,g_2))\n",
        "\t\t# evaluate the model performance every 'epoch'\n",
        "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVHhZ3OVHTm1"
      },
      "source": [
        "## **Run Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a6n0xYsHkKK"
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "discriminator = define_discriminator()\n",
        "# create the generator\n",
        "generator = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train(generator, discriminator, gan_model, dataset, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ajP_u73s6m"
      },
      "source": [
        "# Main Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxUicSPUOP6"
      },
      "source": [
        "### Import and configure modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools\n",
        "\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "# Load compressed models from tensorflow_hub\n",
        "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import shutil\n",
        "import random\n",
        "%matplotlib inline\n",
        "import tensorflow.keras as TFK\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogiec1fohlkT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "model_file_path = \"./drive/MyDrive/models/Zip 94060-Newark_ SFR-Condos-TH.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "FileName = Path('./drive/MyDrive/pythontest.txt')\n",
        "fhand = open(FileName)\n",
        "inp = fhand.read()\n",
        "print(len(inp))\n",
        "#for eachLine in (fhand):\n",
        "#  print(eachLine)\n"
      ],
      "metadata": {
        "id": "wEqBE722lVVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "basepath = Path('./drive/MyDrive/')\n",
        "files_in_basepath = basepath.iterdir()\n",
        "fileCount = 0\n",
        "for item in files_in_basepath:\n",
        "    if item.is_file():\n",
        "      fileCount+= 1\n",
        "print(\"file count is: \",  fileCount)"
      ],
      "metadata": {
        "id": "bpR7HY0XFLh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6msVLevwcRhm"
      },
      "source": [
        "# Python Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc-0LJlrYMFy"
      },
      "source": [
        "myImg = [\n",
        "         [1,0,0,1],\n",
        "         [2,1,1,2],\n",
        "         [3,0,0,1],\n",
        "         [4,1,1,2]\n",
        "         ]\n",
        "#print(\"ConvLu First{0},Second{1}, Rest{other} \".format(myImg[0][0],myImg[0][1],other=myImg[0][2]))\n",
        "#myImgNew = sorted(myImg)\n",
        "for convLu in myImg:\n",
        "  print(\"My Image Data: {}\".format(convLu))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTUQt1zKatjA"
      },
      "source": [
        "#A = np.array([[1,1],[1.5,4.0]])\n",
        "#B= np.array([2200,5050])\n",
        "\n",
        "#x= np.linalg.solve(A,B)\n",
        "#print (x)\n",
        "\n",
        "data = np.genfromtxt(model_file_path,delimiter=',',dtype=np.ndarray)\n",
        "print(data.shape)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nt-VsJ2Cwru"
      },
      "source": [
        "class MyCustomError(Exception):\n",
        "  def __init__(self,message,value):\n",
        "    self.message = message\n",
        "    self.value = value\n",
        "\n",
        "def myErrTest(x):\n",
        "  if x>7:\n",
        "    raise MyCustomError(\"Yz mistake\",x)\n",
        "\n",
        "\n",
        "try:\n",
        "  A= 9\n",
        "  myErrTest(A)\n",
        "except MyCustomError as E:\n",
        "  print(f\"special Error: {E.message}, with this value: {E.value}\")\n",
        "except Exception as e:\n",
        "  print(f\"Some other error:[ {e} ]\")\n",
        "finally:\n",
        "  print(\"Whatever\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYAd2iUdhFp0"
      },
      "source": [
        "import functools\n",
        "\n",
        "def start_end_decorator(num_times):\n",
        "\n",
        "  def repeat(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args,**kwargs):\n",
        "      for _ in range(num_times):\n",
        "        result = func(*args,**kwargs)\n",
        "      return result\n",
        "    return wrapper\n",
        "  return repeat\n",
        "\n",
        "\n",
        "class CountCalls:\n",
        "\n",
        "  def __init__(self,func):\n",
        "    self.func = func\n",
        "    self.num_calls = 0\n",
        "\n",
        "  def __call__(self,*args,**kwargs):\n",
        "    self.num_calls +=1\n",
        "    print (f\"This is executed {self.num_calls} times\")\n",
        "    return self.func(*args,**kwargs)\n",
        "\n",
        "@CountCalls\n",
        "def some_func(x):\n",
        "  print(f\"some func X:{x}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L36TXSiJYL-K"
      },
      "source": [
        "some_func(\"Yathish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ithfP5bawSJO"
      },
      "source": [
        "def mygenerator():\n",
        "  yield 1\n",
        "  yield 2\n",
        "  yield 3\n",
        "\n",
        "g = mygenerator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4qnG3djwfF2"
      },
      "source": [
        "def myFunk():\n",
        "#  global number\n",
        "  number= 9\n",
        "  x = number\n",
        "  print(f\"x is:{x} and n is: {number}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DpoJ486ySwH"
      },
      "source": [
        "org = [5,7]\n",
        "cpy = org\n",
        "cpy[0] = 9\n",
        "cpy[1] = 4\n",
        "org[1] = 2\n",
        "print (f\"origin number is: {org} and new number is: {cpy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TipFuBMy2bPE"
      },
      "source": [
        "x = torch.rand(3,2,requires_grad=True)\n",
        "print(x)\n",
        "y = x*x\n",
        "v = torch.rand(3,2,dtype=torch.float32)\n",
        "y.backward(v)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMhgCCYsr1ka"
      },
      "source": [
        "class BaseMeta(type):\n",
        "  def __new__(cls,name,bases, body):\n",
        "    print(f\"Base Meta = Class: {cls}\\n Name={name}\\n Bases={bases}\\n Body={body}\")\n",
        "    if not 'bar' in body:\n",
        "      print(f\"NO BAEEERRRR\")\n",
        "    else:\n",
        "      print (f\"YYUP BAR\")\n",
        "    return super().__new__(cls,name,bases, body)\n",
        "\n",
        "class Polynomial(metaclass=BaseMeta):\n",
        "  def __init__(self,*coeffs):\n",
        "    self.coeffs = coeffs\n",
        "\n",
        "  def __repr__(self,*coeffs):\n",
        "    return (f\"Poly Repper: {self.coeffs}\")\n",
        "\n",
        "  def __add__(self,other):\n",
        "    return Polynomial (* (x+y for x,y in zip(self.coeffs, other.coeffs)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.coeffs)\n",
        "\n",
        "  def foo(self):\n",
        "    return self.bar()\n",
        "\n",
        "\n",
        "poly1 = Polynomial(1,2,3) # 1st polynomial X² + 2x + 3\n",
        "\n",
        "class DerivedPoly(Polynomial):\n",
        "  def bar(self):\n",
        "    return 'bar'\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmzc0PeDPi46"
      },
      "source": [
        "def Add1(x,y):\n",
        "  return x+y\n",
        "\n",
        "class Adder:\n",
        "  def __call__(self, x, y):\n",
        "    return (f\"X+Y ==> Add2: {x +y}\")\n",
        "\n",
        "Add2 = Adder()\n",
        "\n",
        "print(f\"X+Y=> Add1: {Add1(3,4)}\")\n",
        "print(Add2(3,4))\n",
        "\n",
        "print(type(Add1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDNYO23JuDU8"
      },
      "source": [
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import UpSampling2D\n",
        "# define input data\n",
        "X = asarray([[1, 2],\n",
        "\t\t\t [3, 4]])\n",
        "# show input data for context\n",
        "print(X)\n",
        "# reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "# summarize output\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzGD063jfFY7"
      },
      "source": [
        " # #1 Neural Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM6VEGrGLh62"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeXebYusyHwC"
      },
      "source": [
        "Download images and choose a style image and a content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqc0OJHwyFAk"
      },
      "source": [
        "#content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'file:///content/drive/My Drive/DeepLearning/Data/pr_ro.jpg')\n",
        "#style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','file:///content/drive/My Drive/DeepLearning/Data/Shinol.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "## Visualize the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klh6ObK2t_vH"
      },
      "source": [
        "Define a function to load an image and limit its maximum dimension to 512 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAlRzJZrWM3"
      },
      "source": [
        "Create a simple function to display an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBX-eNT8PAK_"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWQmeEaiKkP"
      },
      "source": [
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMzChXSlKTA2"
      },
      "source": [
        "## Fast Style Transfer using TF-Hub\n",
        "\n",
        "This tutorial demonstrates the original style-transfer algorithm, which optimizes the image content to a particular style. Before getting into the details, let's see how the [TensorFlow Hub model](https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2) does this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYSLexgRKSh-"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEwZ7FlwrjoZ"
      },
      "source": [
        "## Define content and style representations\n",
        "\n",
        "Use the intermediate layers of the model to get the *content* and *style* representations of the image. Starting from the network's input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level features—object parts like *wheels* or *eyes*. In this case, you are using the VGG19 network architecture, a pretrained image classification network. These intermediate layers are necessary to define the representation of content and style from the images. For an input image, try to match the corresponding style and content target representations at these intermediate layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_7zrziuiJk"
      },
      "source": [
        "Load a [VGG19](https://keras.io/applications/#vgg19) and test run it on our image to ensure it's used correctly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMbzrr7BCTq0"
      },
      "source": [
        "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "prediction_probabilities = vgg(x)\n",
        "prediction_probabilities.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_FyCm0dYnvl"
      },
      "source": [
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljpoYk-0f6HS"
      },
      "source": [
        "Now load a `VGG19` without the classification head, and list the layer names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh_AV6220ebD"
      },
      "source": [
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-tASys0eJv"
      },
      "source": [
        "Choose intermediate layers from the network to represent the style and content of the image:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfX_6iA0WAX"
      },
      "source": [
        "content_layers = ['block5_conv3','block5_conv4']\n",
        "\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o4nSwuN0U3X"
      },
      "source": [
        "#### Intermediate layers for style and content\n",
        "\n",
        "So why do these intermediate outputs within our pretrained image classification network allow us to define style and content representations?\n",
        "\n",
        "At a high level, in order for a network to perform image classification (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.\n",
        "\n",
        "This is also a reason why convolutional neural networks are able to generalize well: they’re able to capture the invariances and defining features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classification label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you're able to describe the content and style of input images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3i3RRrJiOX"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "The networks in `tf.keras.applications` are designed so you can easily extract the intermediate layer values using the Keras functional API.\n",
        "\n",
        "To define a model using the functional API, specify the inputs and outputs:\n",
        "\n",
        "`model = Model(inputs, outputs)`\n",
        "\n",
        "This following function builds a VGG19 model that returns a list of intermediate layer outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfec6MuMAbPx"
      },
      "source": [
        "def vgg_layers(layer_names):\n",
        "  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "  # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model([vgg.input], outputs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaIvZf5wWn_"
      },
      "source": [
        "And to create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkyvPpBHSfVi"
      },
      "source": [
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "#Look at the statistics of each layer's output\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "  print(name)\n",
        "  print(\"  shape: \", output.numpy().shape)\n",
        "  print(\"  min: \", output.numpy().min())\n",
        "  print(\"  max: \", output.numpy().max())\n",
        "  print(\"  mean: \", output.numpy().mean())\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGUfttK9F8d5"
      },
      "source": [
        "## Calculate style\n",
        "\n",
        "The content of an image is represented by the values of the intermediate feature maps.\n",
        "\n",
        "It turns out, the style of an image can be described by the means and correlations across the different feature maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. This Gram matrix can be calculated for a particular layer as:\n",
        "\n",
        "$$G^l_{cd} = \\frac{\\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}$$\n",
        "\n",
        "This can be implemented concisely using the `tf.linalg.einsum` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAy1iGPdoEpZ"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  return result/(num_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXIUX6czZABh"
      },
      "source": [
        "## Extract style and content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HGHvwlJ1nkn"
      },
      "source": [
        "Build a model that returns the style and content tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr6QALY-I1ja"
      },
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg =  vgg_layers(style_layers + content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"Expects float input in [0,1]\"\n",
        "    inputs = inputs*255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                      outputs[self.num_style_layers:])\n",
        "\n",
        "    style_outputs = [gram_matrix(style_output)\n",
        "                     for style_output in style_outputs]\n",
        "\n",
        "    content_dict = {content_name:value\n",
        "                    for content_name, value\n",
        "                    in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "    style_dict = {style_name:value\n",
        "                  for style_name, value\n",
        "                  in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "    return {'content':content_dict, 'style':style_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuj1o33t1edl"
      },
      "source": [
        "When called on an image, this model returns the gram matrix (style) of the `style_layers` and content of the `content_layers`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkjO-DoNDU0A"
      },
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "\n",
        "print('Styles:')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    shape: \", output.numpy().shape)\n",
        "  print(\"    min: \", output.numpy().min())\n",
        "  print(\"    max: \", output.numpy().max())\n",
        "  print(\"    mean: \", output.numpy().mean())\n",
        "  print()\n",
        "\n",
        "print(\"Contents:\")\n",
        "for name, output in sorted(results['content'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    shape: \", output.numpy().shape)\n",
        "  print(\"    min: \", output.numpy().min())\n",
        "  print(\"    max: \", output.numpy().max())\n",
        "  print(\"    mean: \", output.numpy().mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9r8Lyjb_m0u"
      },
      "source": [
        "## Run gradient descent\n",
        "\n",
        "With this style and content extractor, you can now implement the style transfer algorithm. Do this by calculating the mean square error for your image's output relative to each target, then take the weighted sum of these losses.\n",
        "\n",
        "Set your style and content target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgkNOnGUFcKa"
      },
      "source": [
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNPrpl-e_w9A"
      },
      "source": [
        "Define a `tf.Variable` to contain the image to optimize. To make this quick, initialize it with the content image (the `tf.Variable` must be the same shape as the content image):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0vKxF8ZO6G8"
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6L8ojmn_6rH"
      },
      "source": [
        "Since this is a float image, define a function to keep the pixel values between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgpTJwL_vE2"
      },
      "source": [
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBU5RFpcAo7W"
      },
      "source": [
        "Create an optimizer. The paper recommends LBFGS, but `Adam` works okay, too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4XZjqUk_5Eu"
      },
      "source": [
        "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As-evbBiA2qT"
      },
      "source": [
        "To optimize this, use a weighted combination of the two losses to get the total loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt4pxarvA4I4"
      },
      "source": [
        "style_weight=1e-2\n",
        "content_weight=1e4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ggx2Na8oROH"
      },
      "source": [
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbF2WnP9BI5M"
      },
      "source": [
        "Use `tf.GradientTape` to update the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t0umkajFIuh"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FHMJq4UBRIQ"
      },
      "source": [
        "Now run a few steps to test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y542mxi-O2a2"
      },
      "source": [
        "train_step(image)\n",
        "train_step(image)\n",
        "train_step(image)\n",
        "tensor_to_image(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNzE-mTbBVgY"
      },
      "source": [
        "Since it's working, perform a longer optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQW1tXYoLbUS"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 20\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"Train step: {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVB3anJMY2v"
      },
      "source": [
        "## Total variation loss\n",
        "\n",
        "One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the *total variation loss*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7szUUybCQMB3"
      },
      "source": [
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atc2oL29PXu_"
      },
      "source": [
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.subplot(2,2,1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqHElVgBkgkz"
      },
      "source": [
        "This shows how the high frequency components have increased.\n",
        "\n",
        "Also, this high frequency component is basically an edge-detector. You can get similar output from the Sobel edge detector, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyvqCiywiUfL"
      },
      "source": [
        "plt.figure(figsize=(14,10))\n",
        "\n",
        "sobel = tf.image.sobel_edges(content_image)\n",
        "plt.subplot(1,2,1)\n",
        "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
        "plt.subplot(1,2,2)\n",
        "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv5bKlSDnPP7"
      },
      "source": [
        "The regularization loss associated with this is the sum of the squares of the values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP-92lXMIYPn"
      },
      "source": [
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4OYBUX2KQ25"
      },
      "source": [
        "total_variation_loss(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu2hJ8zOKMc1"
      },
      "source": [
        "That demonstrated what it does. But there's no need to implement it yourself, TensorFlow includes a standard implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQjWW04NKLfJ"
      },
      "source": [
        "tf.image.total_variation(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTessd-DCdcC"
      },
      "source": [
        "## Re-run the optimization\n",
        "\n",
        "Choose a weight for the `total_variation_loss`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGeRLD4GoAd4"
      },
      "source": [
        "total_variation_weight=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1-T4kJsoAv"
      },
      "source": [
        "Now include it in the `train_step` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmfcyyYUyWq"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    loss += total_variation_weight*tf.image.total_variation(image)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcLWBQChsutQ"
      },
      "source": [
        "Reinitialize the optimization variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dPRr8BqexB"
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEflRstmtGBu"
      },
      "source": [
        "And run the optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Cc3bLtoOWy"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"Train step: {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKox7K46tKxy"
      },
      "source": [
        "Finally, save the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSH6OpyyQn7w"
      },
      "source": [
        "file_name = 'stylized-image.png'\n",
        "tensor_to_image(image).save(file_name)\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KiJHGho1TMe"
      },
      "source": [
        "# Customize VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EgRCeO71NTw"
      },
      "source": [
        "vgg_model = TFK.applications.vgg16.VGG16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4dmA682ghv"
      },
      "source": [
        "YS_vgg_model = TFK.Sequential()\n",
        "for layer in vgg_model.layers:\n",
        "  YS_vgg_model.add(layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVoxrXo53yP0"
      },
      "source": [
        "for layer in YS_vgg_model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYmoKVTN4fOF"
      },
      "source": [
        "YS_vgg_model.add(TFK.layers.Dense(2,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKh858TN5jSr"
      },
      "source": [
        "YS_vgg_model.compile(TFK.optimizers.Adam(learning_rate=0.0001),loss=TFK.losses.categorical_crossentropy, metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSJVVjjC8mEq"
      },
      "source": [
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','file:///content/drive/My Drive/vangogh.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C35qI15lgI19"
      },
      "source": [
        "# DL - Cat & Dog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN1TwW5YgRsw"
      },
      "source": [
        "train_path = '/content/drive/My Drive/DeepLearning/Deep_Liz/data/dogs-vs-cats/train'\n",
        "valid_path = '/content/drive/My Drive/DeepLearning/Deep_Liz/data/dogs-vs-cats/valid'\n",
        "test_path = '/content/drive/My Drive/DeepLearning/Deep_Liz/data/dogs-vs-cats/test'\n",
        "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
        "    directory=train_path, target_size=(224,224), batch_size=10)\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
        "    directory=valid_path, target_size=(224,224), batch_size=10)\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
        "    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKd9ZroylvkF"
      },
      "source": [
        "assert train_batches.n == 40\n",
        "valid_batches.n == 16\n",
        "test_batches.n == 10\n",
        "train_batches.num_classes == valid_batches.num_classes == test_batches.num_classes == 2\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utxlVkWwlwjA"
      },
      "source": [
        "mobile = tf.keras.applications.mobilenet.MobileNet()\n",
        "mobile.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXOOz-UmAqL"
      },
      "source": [
        "def count_params(model):\n",
        "    non_trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.non_trainable_weights])\n",
        "    trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_weights])\n",
        "    return {'non_trainable_params': non_trainable_params, 'trainable_params': trainable_params}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vai6NzAumDBe"
      },
      "source": [
        "params = count_params(mobile)\n",
        "assert params['non_trainable_params'] == 21888\n",
        "assert params['trainable_params'] == 4231976"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs25Vk1mmFVy"
      },
      "source": [
        "x = mobile.layers[-6].output\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "model = Model(inputs=mobile.input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-x5_ClmJOD"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfb9mhyMmLlh"
      },
      "source": [
        "params = count_params(model)\n",
        "assert params['non_trainable_params'] == 21888\n",
        "assert params['trainable_params'] == 3209026"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSNevRIdmPgb"
      },
      "source": [
        "for layer in model.layers[:-5]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCJdvvZXmSu5"
      },
      "source": [
        "params = count_params(model)\n",
        "assert params['non_trainable_params'] == 2178240\n",
        "assert params['trainable_params'] == 1052674"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp4TaLNNmcXO"
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=train_batches, steps_per_epoch=4, validation_data=valid_batches, validation_steps=2, epochs=10, verbose=2)\n",
        "assert model.history.history.get('accuracy')[-1] > 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUs0RGgeM4tO"
      },
      "source": [
        "test_labels = np.random.choice(test_batches.classes,10)\n",
        "test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4URJjk9ql3T"
      },
      "source": [
        "predictions = model.predict(x=test_batches, steps=1, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmrKjKBZSoIB"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46i9r2W7qqcn"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H-xQ2UXqrJn"
      },
      "source": [
        "cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa-JMLvequbP"
      },
      "source": [
        "cm_plot_labels = ['cat','dog']\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}